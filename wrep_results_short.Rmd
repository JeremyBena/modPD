---
output:
  pdf_document:
  html_document:
    fig_caption: yes
    keep_md: yes
editor_options:
  chunk_output_type: console
---

```{r set, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,cache=TRUE,message=FALSE,warning=FALSE)
options(scipen=1, digits=2)

#Load (or install and load) packages
require(pacman)
p_load('tidyverse', 'psych', 'effectsize', 'afex', 'lme4', 'lmerTest', 'effects', 'sjstats', 'sjPlot', 'ggeffects', 'bfrr', 'papaja') 
set_sum_contrasts()

#read the dataset we created in a previous R script
dat = readRDS("data/data_wrep_final.RDS") #40 rows for each participant (because 40 different CSs)

#the "eval_condition" factor is not very helpful as it confounds two factors (order and tasks); separate the two
dat = dat %>% separate(eval_condition, c("eval_task","eval_task_order"), sep = "_ratings_")

#some factors are integer or character variables in the dataset; make them factors
dat$subject = as.factor(dat$subject)
dat$us_valence = as.factor(dat$us_valence)
dat$eval_task = as.factor(dat$eval_task)
dat$eval_task_order = as.factor(dat$eval_task_order)
dat$failed_check_tbs = as.factor(dat$failed_check_tbs)
dat$failed_check_vma = as.factor(dat$failed_check_vma)
dat$failed_check_3bs = as.factor(dat$failed_check_3bs)
```

```{r, exclude_instruction_checks, include=FALSE}
#exclude participants that failed instruction-understanding checks

#how much participants failed the tbs, vma, or 3bs?
# table(dat$failed_check_tbs)/40
# table(dat$failed_check_vma)/40
# table(dat$failed_check_3bs)/40

dat = dat %>% filter((failed_check_vma=="not_failed" & eval_task == "Wrep") |
                    (failed_check_3bs == "not_failed" & eval_task == "3BS_study") |
                    (failed_check_tbs == "not_failed" & eval_task == "TBSrep")
                    )

addmargins(table(dat$eval_task)/40)

length(unique(dat$subject)) # N of participants after instruction-understanding check exclusion
```

```{r count, include=FALSE}
#exclude participants declaring they did not take their responses seriously
##or did not pay attention
dat = dat %>% filter(pay_attention != 0 & serious != 0) %>% droplevels() #drop level to remove excluded ppts
length(unique(dat$subject)) # N of participants after exclusion
#185 participants kept after data exclusion 

#How many participant in each evaluative condition after data exclusion?
addmargins(table(dat$eval_task)/40)
```

## Results

We used both frequentist and Bayesian analyses. We computed Bayes Factors to quantify the evidence for the alternative (H1) against the null (H0) hypothesis. As per Waroquier et al., H1 was modeled as a half-normal distribution with an SD of 15% when testing whether valence identification was above chance, and with an SD of 23.5 evaluative rating units when testing evaluative conditioning (EC) effects. We used conventional cut-offs to interpret these Bayes Factors (noted $BF_{H(0, SD)}$; e.g., Dienes, 2014). A BF above 3 yields evidence for H1 compared to H0, while a BF below 1/3 yields evidence for H0. A BF between 1/3 and 3 would count as anecdotal evidence in either direction, and indicates inconclusive data (i.e., no evidence for the hypotheses).

### Overall evaluative conditioning effect

First, we tested whether there was an EC effect overall (regardless of the specific task condition), based on the final total sample (*N =* `r length(unique(dat$subject))`). We computed pre-post evaluative change scores and tested whether they were influenced by US Valence (see Figure \@ref(fig:plot_eval_change)). 

```{r eval_change, include=FALSE}
####
#EVALUATIVE CHANGE IN GENERAL
####

#compute evaluative change scores
#first, rescale the prerating and postrating scales (0 to 400 instead of -200 to 200)
dat$scale_prerating = dat$scale_prerating + 200
dat$scale_postrating = dat$scale_postrating + 200

#compute the difference between pre- and post-ratings for each CS
dat_eval_change = dat %>% 
  group_by(subject, c, u, us_valence) %>% 
  summarise(eval_change_score = scale_postrating - scale_prerating)

#add this evaluative change score to the dataset
dat = full_join(dat, dat_eval_change,
                , by = c("subject"="subject", "us_valence"="us_valence", "c"="c", "u"="u"))

#compute mean evaluative change scores for each participant as a function of US Valence 
dat_ev = dat %>%
  group_by(subject, us_valence) %>%
  summarise(mean_eval_change = mean(eval_change_score))

#check data distribution
# plot(density(dat_ev$mean_eval_change[dat_ev$us_valence=="positive"]))
# plot(density(dat_ev$mean_eval_change[dat_ev$us_valence=="negative"]))
```

```{r eval_change_test, include=FALSE}
#test the difference in a repeated-measures ANOVA (could also be done in a t-test)
mod1 = aov_ez(dat_ev
              ,id = "subject"
              ,dv = "mean_eval_change"
              ,within = "us_valence"
)

# mod1_print = apa_print(mod1)

# apa_table(
#   mod1_print$table
#   ,caption = "Repeated-measures ANOVA: Evaluative change scores as a function of US Valence irrespective of Task condition"
# )

# Descriptive statistics: evaluative change scores as a function of US Valence
# knitr::kable(describeBy(dat_ev$mean_eval_change, dat_ev$us_valence, mat = TRUE), digits = 2)

#to compute Bayes Factors similarly to how Waroquier et al. did, we will compute the difference of evaluative change scores between positive and negative USs

#first, make a wide data frame so that we can compute the differences
dat_ev_wide = dat_ev %>% pivot_wider(names_from = "us_valence"
                                     ,values_from = "mean_eval_change")

#compute the difference
dat_ev_wide$diff_overall = dat_ev_wide$positive-dat_ev_wide$negative

#use the bfrr package to compute Bayes Factors. The package is helpful because it allows one to
##conveniently specify H1 and to compute robustness regions, as in Waroquier et al.
bf_ev_overall = bfrr(
  sample_mean = mean(dat_ev_wide$diff_overall), # mean of the sample
  sample_se = sd(dat_ev_wide$diff_overall)/sqrt(length(dat_ev_wide$diff_overall)), # SE of the sample
  sample_df = length(dat_ev_wide$diff_overall) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = 23.5, #as in Waroquier et al.
  tail = 1, #one sided 
  criterion = 3, 
  rr_interval = list(
    # mean = c(-2, 2),
    sd = c(0, 500)
  )
)
```

```{r plot_eval_change, fig.cap="Pre-post evaluative change as a function of US Valence irrespective of Task condition. Dots are the individual observations, and error bars are the 95% Confidence Intervals"}
#visualize the data
apa_beeplot(data=dat, id="subject", dv="eval_change_score", factors=c( "us_valence"), intercept =0, use = "all.obs", ylim=c(-200,200),
            xlab = "US Valence"
            ,ylab = "Evaluative change score")
```

Overall, we found an EC effect: pre-post evaluative change scores were higher for CSs paired with positive than negative USs, `r apa_print(mod1)$full_result`, $BF_{H(0, 23.5)} =$ `r bf_ev_overall$BF`.

### Two Buttons Sets (TBS) task

```{r tbs_count, include=FALSE}
####
#TBS PROCEDURE
####

#keep only participants that performed the TBS
dat_tbs = dat %>% filter(eval_task == "TBSrep") %>% droplevels()

##alternatively, we can keep only participants that performed the TBS as the first or second evaluative task (the TBS condition)
#dat_tbs = dat %>% filter(eval_task == "TBSrep") %>% droplevels()

#length(unique(dat_tbs$subject)) #127 participants

#recode the tbs response into two variables: whether participant gave a positive or negative response (tbs_val_resp)
##and whether participants used the memory of the attitude button set (tbs_button_resp)
dat_tbs$response_tbs = str_remove(dat_tbs$response_tbs, "_response")
dat_tbs = dat_tbs %>% separate(response_tbs, c("tbs_val_resp","tbs_button_resp"))

dat_tbs$tbs_button_resp = as.factor(dat_tbs$tbs_button_resp)

#count memory and attitude button responses
# n_attrib_tbs = dat_tbs %>% 
#   group_by(subject, tbs_button_resp, .drop=FALSE) %>% tally()
# colnames(n_attrib_tbs) = c("subject", "attrib_buttons", "n_attrib_tbs")

#at the aggregated level
#knitr::kable(describeBy(n_attrib_tbs$n, n_attrib_tbs$tbs_button_resp, mat=TRUE), digits=2)
```

```{r tbs_correct_proportions_overall, include=FALSE}
#for each participant, compute the proportion of correct identifications in the TBS
#meaning, say "positive" if the CS was paired with a positive US, and say "negative if the CS was paired with a negative US
dat_tbs$tbs_correct = as.factor(ifelse(substr(dat_tbs$us_valence, 1, 3) == dat_tbs$tbs_val_resp, "correct", "incorrect"))

prop_correct_tbs = dat_tbs %>% 
  group_by(subject, tbs_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(subject) %>% filter(!is.nan(freq))

colnames(prop_correct_tbs) = c("subject", "correct_response", "n_count", "prop")

#at the aggregated level
#knitr::kable(describe(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"]), digits=2)

#tests
t_prop_correct_o = t.test(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"], mu = 0.5)
d_prop_correct_o = cohens_d(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"], mu = .5)

bf_prop_overall_tbs = bfrr(
  sample_mean = mean(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"])-.5, # mean of the sample
  sample_se = sd(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"])/sqrt(length(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)
#summary(bf_prop_overall_tbs)
```

```{r tbs_correct_proportions_memory, include=FALSE}
#proportion correct only for memory buttons in the TBS
prop_correct_tbs_mem = dat_tbs %>% filter(tbs_button_resp == "memory") %>%
  group_by(subject, tbs_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(subject) %>% filter(!is.nan(freq))

colnames(prop_correct_tbs_mem) = c("subject", "correct_response", "n_count", "prop")

#at the aggregated level
#knitr::kable(describe(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"]), digits=2)

#tests
t_prop_correct_m = t.test(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"], mu = 0.5)
d_prop_correct_m = cohens_d(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"], mu = .5)

bf_prop_tbs_memory = bfrr(
  sample_mean = mean(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])-.5, # mean of the sample
  sample_se = sd(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])/sqrt(length(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)
#summary(bf_prop_tbs_memory)
```

```{r tbs_correct_proportions_attitudes, include=FALSE}
#proportion correct only for attitude buttons in the TBS
prop_correct_tbs_att = dat_tbs %>% filter(tbs_button_resp == "attitude") %>%
  group_by(subject, tbs_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(subject) %>% filter(!is.nan(freq))

colnames(prop_correct_tbs_att) = c("subject", "correct_response", "n_count", "prop")

#at the aggregated level
#knitr::kable(describe(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"]), digits=2)

#tests
t_prop_correct_a = t.test(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"], mu = 0.5)
d_prop_correct_a = cohens_d(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"], mu = .5)

bf_prop_tbs_att = bfrr(
  sample_mean = mean(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])-.5, # mean of the sample
  sample_se = sd(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])/sqrt(length(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)
#summary(bf_prop_tbs_att)
```

#### Valence memory in the TBS task

We conducted analyses with data from participants in the TBS task condition (*n =* `r length(unique(dat_tbs$subject))`). We tested whether valence memory performance was higher than chance (50%) overall and separately for each response buttons set. Overall, valence memory as estimated in the TBS task was above chance (*M =* `r mean(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"])`; *SD =* `r sd(prop_correct_tbs$prop[prop_correct_tbs$correct_response=="correct"])`), `r apa_print(t_prop_correct_o)$statistic`, *d =* `r d_prop_correct_o$Cohens_d`, $BF_{H(0, 15\%)}=$ `r bf_prop_overall_tbs$BF`. This was also the case when participants used the "memory" buttons set (*M =* `r mean(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])`; *SD =* `r sd(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])`), `r apa_print(t_prop_correct_m)$statistic`, *d =* `r d_prop_correct_m$Cohens_d`, $BF_{H(0, 15\%)}=$ `r bf_prop_tbs_memory$BF`. Importantly, valence memory performance was not different from the chance level for responses on the attitude buttons set (*M =* `r mean(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])`; *SD =* `r sd(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])`), `r apa_print(t_prop_correct_a)$statistic`, *d =* `r d_prop_correct_a$Cohens_d`, $BF_{H(0, 15\%)}=$ `r bf_prop_tbs_att$BF`.

```{r prepare_plot_tbs, include=FALSE}
dat_tbs$tbs_button_resp = fct_relevel(dat_tbs$tbs_button_resp, "memory", "attitude")

dat_tbs_plot = dat_tbs %>%
  group_by(subject, us_valence, tbs_button_resp) %>%
  summarise(mean_eval_change = mean(eval_change_score)) 

dat_tbs_plot_wide = dat_tbs_plot %>% pivot_wider(names_from = us_valence
                                                 ,values_from = mean_eval_change)

dat_tbs_plot_wide$EC_score = dat_tbs_plot_wide$positive - dat_tbs_plot_wide$negative
```

```{r overall_tbs_figure, fig.cap="Difference between evaluative change scores for CS+ and CS- (EC scores) as a function of Response button sets (TBS Task condition)"}
apa_beeplot(data=dat_tbs_plot_wide, id="subject", dv="EC_score", factors="tbs_button_resp", intercept =0, use = "all.obs", ylim=c(-200,200), xlab = 'TBS response button set', ylab = 'Evaluative Conditioning effect score')
```

```{r tbs_eval_change_memory, include=FALSE}
####
#EVALUATIVE CHANGE AS A FUNCTION OF VALENCE MEMORY ATTRIBUTIONS IN THE TBS TASK
####

###memory button
dat_ev_tbs_memory = dat_tbs %>%  filter(tbs_button_resp == "memory") %>%
  group_by(subject, us_valence) %>%
  summarise(mean_eval_change = mean(eval_change_score)) %>% filter(n() > 1)

mod_memory_tbs = aov_ez(dat_ev_tbs_memory
                                ,id = "subject"
                                ,dv = "mean_eval_change"
                                ,within = "us_valence"
)

# mod_memory_tbs_print = apa_print(mod_memory_tbs)
# 
# apa_table(
#   mod_memory_tbs_print$table
#   ,caption = "Repeated-measures ANOVA: Evaluative change scores as a function of US Valence for CSs that received a memory-buttons set response (TBS task)"
# )
# 
# knitr::kable(describeBy(dat_ev_tbs_memory$mean_eval_change, dat_ev_tbs_memory$us_valence, mat=TRUE), digits=2)

dat_ev_tbs_memory_wide = dat_ev_tbs_memory %>% pivot_wider(names_from = "us_valence"
                                                                           ,values_from = "mean_eval_change")

dat_ev_tbs_memory_wide$diff_overall = dat_ev_tbs_memory_wide$positive-dat_ev_tbs_memory_wide$negative 

dat_ev_tbs_memory_wide = dat_ev_tbs_memory_wide %>% filter(!is.na(diff_overall))

bf_ev_memory_tbs = bfrr(
  sample_mean = mean(dat_ev_tbs_memory_wide$diff_overall), # mean of the sample
  sample_se = sd(dat_ev_tbs_memory_wide$diff_overall)/sqrt(length(dat_ev_tbs_memory_wide$diff_overall)), # SE of the sample
  sample_df = length(dat_ev_tbs_memory_wide$diff_overall) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = 23.5,
  tail = 1,
  criterion = 3, 
  rr_interval = list(
    # mean = c(-2, 2),
    sd = c(0, 500)
  )
)

#summary(bf_ev_memory_tbs)
```

```{r tbs_eval_change_attitude, include=FALSE}
###attitude button
dat_ev_tbs_attitude = dat_tbs %>%  filter(tbs_button_resp == "attitude") %>%
  group_by(subject, us_valence) %>%
  summarise(mean_eval_change = mean(eval_change_score)) %>% filter(n() > 1)

mod_attitude_tbs = aov_ez(dat_ev_tbs_attitude
                                  ,id = "subject"
                                  ,dv = "mean_eval_change"
                                  ,within = "us_valence"
)

# mod_attitude_tbs_print = apa_print(mod_attitude_tbs)
# 
# apa_table(
#   mod_attitude_tbs_print$table
#   ,caption = "Repeated-measures ANOVA: Evaluative change scores as a function of US Valence for CSs that received an attitude-buttons set response (TBS task)"
# )
# 
# 
# knitr::kable(describeBy(dat_ev_tbs_attitude$mean_eval_change, dat_ev_tbs_attitude$us_valence, mat=TRUE), digits=2)

dat_ev_tbs_attitude_wide = dat_ev_tbs_attitude %>% pivot_wider(names_from = "us_valence"
                                                                               ,values_from = "mean_eval_change")

dat_ev_tbs_attitude_wide$diff_overall = dat_ev_tbs_attitude_wide$positive-dat_ev_tbs_attitude_wide$negative 

dat_ev_tbs_attitude_wide = dat_ev_tbs_attitude_wide %>% filter(!is.na(diff_overall))

bf_ev_attitude_tbs = bfrr(
  sample_mean = mean(dat_ev_tbs_attitude_wide$diff_overall), # mean of the sample
  sample_se = sd(dat_ev_tbs_attitude_wide$diff_overall)/sqrt(length(dat_ev_tbs_attitude_wide$diff_overall)), # SE of the sample
  sample_df = length(dat_ev_tbs_attitude_wide$diff_overall) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = 23.5,
  tail = 1,
  criterion = 3, 
  rr_interval = list(
    # mean = c(-2, 2),
    sd = c(0, 500)
  )
)

#summary(bf_ev_attitude_tbs)

#mixed-effects logistic regression

dat_tbs$tbs_val_resp_bin = recode(dat_tbs$tbs_val_resp, "pos"=1, "neg"=0)
mixedmod_val = glmer(tbs_val_resp_bin ~ us_valence*tbs_button_resp + (1 + us_valence|subject), family = binomial, data=dat_tbs, control=glmerControl(optimizer = "bobyqa"))

#button set: memory
mixedmod_val_mem = glmer(tbs_val_resp_bin ~ us_valence + (1 + us_valence|subject), family = binomial, data=subset(dat_tbs, tbs_button_resp=="memory"), control=glmerControl(optimizer = "bobyqa"))

#button set: attitude
mixedmod_val_att = glmer(tbs_val_resp_bin ~ us_valence + (1 + us_valence|subject), family = binomial, data=subset(dat_tbs, tbs_button_resp=="attitude"), control=glmerControl(optimizer = "bobyqa"))

#dat_tbs$tbs_button_resp_bin = recode(dat_tbs$tbs_button_resp, "memory"=1, "attitude"=0)
#mixedmod_set = glmer(tbs_button_resp_bin ~ us_valence + (1 + us_valence|subject), family = binomial, data=dat_tbs, control=glmerControl(optimizer = "bobyqa"))

```

#### EC effect in the absence of memory?

Our critical test is whether an EC effect is found when participants lack memory (i.e., when they use the attitude buttons set). The main results are displayed in Figure \@ref(fig:overall_tbs_figure). Both frequentist and Bayesian analyses yielded evidence for an EC effect when participants used the memory buttons set, `r apa_print(mod_memory_tbs)$full_result`, $BF_{H(0, 23.5)}=$ `r bf_ev_memory_tbs$BF`. When participants used the attitude buttons set, the EC effect was not significant and BF yields evidence against the EC effect, `r apa_print(mod_attitude_tbs)$full_result`, $BF_{H(0, 23.5)}=$ `r bf_ev_attitude_tbs$BF`. 

To analyze evaluations participants made in the TBS task, we additionally conducted a mixed-effect logistic regression (random intercepts and slopes for US Valence) with US Valence and Response buttons set as predictors. The interaction between US Valence and Response buttons set was significant, `r apa_print(mixedmod_val)$full_result$us_valence1_tbs_button_resp1`. We conducted separate analyses at each Response buttons set level to simplify interpretation of the findings. When participants used the memory response buttons set, the EC effect was significant, `r apa_print(mixedmod_val_mem)$full_result$us_valence1`. In contrast, in the attitude response buttons set, no significant effect of US Valence was found, `r apa_print(mixedmod_val_att)$full_result$us_valence1`.

### Valence Memory Attribution task 

```{r vma, include=FALSE}
####
#VMA TASK (whether it was administered first or second) - mimics analyses of Waroquier et al.
####

dat_vma = dat %>% filter(eval_task=="Wrep") %>% droplevels()

#length(unique(dat_vma$subject)) #208 participants
```

```{r vma_correct_proportions_overall, include=FALSE}
#for each participant, compute the proportion of correct identifications in the VMA
#meaning, say "positive" if the CS was paired with a positive US, and say "negative if the CS was paired with a negative US
dat_vma$vma_correct = as.factor(ifelse(substr(dat_vma$response_valence_id, 1, 3) == substr(dat_vma$us_valence, 1, 3), "correct", "incorrect"))

prop_correct_vma = dat_vma %>% 
  group_by(subject, vma_correct, .drop=FALSE) %>%  #drop=FALSE to keep "correct" and "incorrect" rows for all participants
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(subject) %>% filter(!is.nan(freq))

#filter "NaN" because it does not make sense to include participants that did not provide responses (concerns mainly "guess" responses)

colnames(prop_correct_vma) = c("subject", "correct_response", "n_count", "prop")

#at the aggregated level
#knitr::kable(describe(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"]), digits = 2)

#tests
t_prop_vma_o = t.test(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"], mu = 0.5)
d_prop_vma_o = cohens_d(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"], mu = .5)

bf_prop_overall = bfrr(
  sample_mean = mean(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"])-.5, # mean of the sample minus the test value .5
  sample_se = sd(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"])/sqrt(length(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
  )
#summary(bf_prop_overall)
```

```{r vma_correct_proportions_memory, include=FALSE}
#proportion correct only for memory attributions in the VMA
prop_correct_vma_mem = dat_vma %>% filter(attrib_buttons == "memory_attrib_button") %>%
  group_by(subject, vma_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(subject) %>% filter(!is.nan(freq))

colnames(prop_correct_vma_mem) = c("subject", "correct_response", "n_count", "prop")

#at the aggregated level
#knitr::kable(describe(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"]), digits = 2)

#tests
t_prop_vma_m = t.test(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"], mu = 0.5)
d_prop_vma_m = cohens_d(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"], mu = .5)

bf_prop_memory = bfrr(
  sample_mean = mean(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"])-.5, # mean of the sample minus the test value .5
  sample_se = sd(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"])/sqrt(length(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)

#summary(bf_prop_memory)
```

```{r vma_correct_proportions_feeling, include=FALSE}
#proportion correct only for feeling attributions in the VMA
prop_correct_vma_feel = dat_vma %>% filter(attrib_buttons == "intuition_feeling_button") %>%
  group_by(subject, vma_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(subject) %>% filter(!is.nan(freq))

colnames(prop_correct_vma_feel) = c("subject", "correct_response", "n_count", "prop")

#at the aggregated level
#knitr::kable(describe(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"]), digits = 2)

#tests
t_prop_vma_f = t.test(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"], mu = 0.5)
d_prop_vma_f = cohens_d(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"], mu = .5)

bf_prop_feel = bfrr(
  sample_mean = mean(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"])-.5, # mean of the sample minus the test value .5
  sample_se = sd(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"])/sqrt(length(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)
#summary(bf_prop_feel)
```

```{r vma_correct_proportions_guessing, include=FALSE}
#proportion correct only for guess attributions in the VMA
prop_correct_vma_guess = dat_vma %>% filter(attrib_buttons == "guess_attrib_button") %>%
  group_by(subject, vma_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(subject) %>% filter(!is.nan(freq))

colnames(prop_correct_vma_guess) = c("subject", "correct_response", "n_count", "prop")

#at the aggregated level
#knitr::kable(describe(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"]), digits = 2)

#tests
t_prop_vma_g = t.test(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"], mu = 0.5)
d_prop_vma_g = cohens_d(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"], mu = .5)

bf_prop_guess = bfrr(
  sample_mean = mean(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"])-.5, # mean of the sample minus the test value .5
  sample_se = sd(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"])/sqrt(length(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)
#summary(bf_prop_guess)
```

#### Valence memory in the VMA task

We conducted analyses with data from participants in the VMA task condition (*n =* `r length(unique(dat_vma$subject))`). Overall, valence memory was above chance (*M =* `r mean(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"])`; *SD =* `r sd(prop_correct_vma$prop[prop_correct_vma$correct_response=="correct"])`), `r apa_print(t_prop_vma_o)$statistic`, *d =* `r d_prop_vma_o$Cohens_d`, $BF_{H(0, 15\%)}=$ `r bf_prop_overall$BF`. This was also the case when participants made a memory attribution (*M =* `r mean(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"])`; *SD =* `r sd(prop_correct_vma_mem$prop[prop_correct_vma_mem$correct_response=="correct"])`), `r apa_print(t_prop_vma_m)$statistic`, *d =* `r d_prop_vma_m$Cohens_d`, $BF_{H(0, 15\%)}=$ `r bf_prop_memory$BF` and when they made an intuition attribution (*M =* `r mean(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"])`; *SD =* `r sd(prop_correct_vma_feel$prop[prop_correct_vma_feel$correct_response=="correct"])`), `r apa_print(t_prop_vma_f)$statistic`, *d =* `r d_prop_vma_f$Cohens_d`, $BF_{H(0, 15\%)}=$ `r bf_prop_feel$BF`. 

Valence memory performance was not significantly different from chance for random attributions (*M =* `r mean(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"])`; *SD =* `r sd(prop_correct_vma_guess$prop[prop_correct_vma_guess$correct_response=="correct"])`), `r apa_print(t_prop_vma_g)$statistic`, *d =* `r d_prop_vma_g$Cohens_d`, but BF yielded inconclusive evidence, $BF_{H(0, 15\%)}=$ `r bf_prop_guess$BF`. 

```{r overall_vma_figure, fig.cap="Difference between evaluative change scores for CS+ and CS- (EC scores) as a function of Attributions (VMA Task condition)"}
dat_vma$attrib_plot = as.factor(recode(dat_vma$attrib_buttons, "guess_attrib_button"="random guessing"
       ,"intuition_feeling_button"="intuition"
       ,"memory_attrib_button"="memory"))

dat_vma$attrib_plot = fct_relevel(dat_vma$attrib_plot, "memory", "intuition", "random guessing")

dat_vma_plot = dat_vma %>%
  group_by(subject, us_valence, attrib_plot) %>%
  summarise(mean_eval_change = mean(eval_change_score)) 

dat_vma_plot_wide = dat_vma_plot %>% pivot_wider(names_from = us_valence
                                                 ,values_from = mean_eval_change)

dat_vma_plot_wide$EC_score = dat_vma_plot_wide$positive - dat_vma_plot_wide$negative

apa_beeplot(data=dat_vma_plot_wide, id="subject", dv="EC_score", factors="attrib_plot", intercept =0, use = "all.obs", ylim=c(-200,200)
            ,xlab = "Evaluative Conditioning effect score"
            ,ylab="Attribution")
```

```{r vma_eval_change_memory, include=FALSE}
####
#EVALUATIVE CHANGE AS A FUNCTION OF VALENCE MEMORY ATTRIBUTIONS IN THE VMA TASK
####

#for each attribution
###memory
dat_ev_vma_memory = dat_vma %>% filter(attrib_buttons == "memory_attrib_button") %>%
  group_by(subject, us_valence) %>%
  summarise(mean_eval_change = mean(eval_change_score)) %>% filter(n() > 1)

mod_memory_vma = aov_ez(dat_ev_vma_memory
                                ,id = "subject"
                                ,dv = "mean_eval_change"
                                ,within = "us_valence"
)

# mod_memory_vma_print = apa_print(mod_memory_vma)
# 
# apa_table(
#   mod_memory_vma_print$table
#   ,caption = "Repeated-measures ANOVA: Evaluative change scores as a function of US Valence for CSs that received a 'Memory' attribution in the Valence Memory Attribution task"
# )
# 
# knitr::kable(describeBy(dat_ev_vma_memory$mean_eval_change, dat_ev_vma_memory$us_valence, mat=TRUE), digits = 2)

dat_ev_vma_memory_wide = dat_ev_vma_memory %>% pivot_wider(names_from = "us_valence"
                                                                           ,values_from = "mean_eval_change")

dat_ev_vma_memory_wide$diff_overall = dat_ev_vma_memory_wide$positive-dat_ev_vma_memory_wide$negative 

dat_ev_vma_memory_wide = dat_ev_vma_memory_wide %>% filter(!is.na(diff_overall))

bf_ev_memory = bfrr(
  sample_mean = mean(dat_ev_vma_memory_wide$diff_overall), # mean of the sample
  sample_se = sd(dat_ev_vma_memory_wide$diff_overall)/sqrt(length(dat_ev_vma_memory_wide$diff_overall)), # SE of the sample
  sample_df = length(dat_ev_vma_memory_wide$diff_overall) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = 23.5,
  tail = 1,
  criterion = 3, 
  rr_interval = list(
    # mean = c(-2, 2),
    sd = c(0, 500)
  )
)

#summary(bf_ev_memory)
```

```{r vma_eval_change_intuition, include=FALSE}
###feeling
dat_ev_vma_feeling = dat_vma %>%  filter(attrib_buttons == "intuition_feeling_button") %>%
  group_by(subject, us_valence) %>%
  summarise(mean_eval_change = mean(eval_change_score)) %>% filter(n() > 1)

mod_feeling_vma = aov_ez(dat_ev_vma_feeling
                                 ,id = "subject"
                                 ,dv = "mean_eval_change"
                                 ,within = "us_valence"
)

# mod_feeling_vma_print = apa_print(mod_feeling_vma)
# 
# apa_table(
#   mod_feeling_vma_print$table
#   ,caption = "Repeated-measures ANOVA: Evaluative change scores as a function of US Valence for CSs that received an 'Intuition' attribution in the Valence Memory Attribution task"
# )
# 
# knitr::kable(describeBy(dat_ev_vma_feeling$mean_eval_change, dat_ev_vma_feeling$us_valence, mat=TRUE), digits=2)

dat_ev_vma_feeling_wide = dat_ev_vma_feeling %>% pivot_wider(names_from = "us_valence"
                                                                             ,values_from = "mean_eval_change")

dat_ev_vma_feeling_wide$diff_overall = dat_ev_vma_feeling_wide$positive-dat_ev_vma_feeling_wide$negative 

dat_ev_vma_feeling_wide = dat_ev_vma_feeling_wide %>% filter(!is.na(diff_overall))

bf_ev_feeling = bfrr(
  sample_mean = mean(dat_ev_vma_feeling_wide$diff_overall), # mean of the sample
  sample_se = sd(dat_ev_vma_feeling_wide$diff_overall)/sqrt(length(dat_ev_vma_feeling_wide$diff_overall)), # SE of the sample
  sample_df = length(dat_ev_vma_feeling_wide$diff_overall) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = 23.5,
  tail = 1,
  criterion = 3, 
  rr_interval = list(
    # mean = c(-2, 2),
    sd = c(0, 500)
  )
)

#summary(bf_ev_feeling)
```

```{r vma_eval_change_guessing, include=FALSE}
###guess
dat_ev_vma_guess = dat_vma %>% filter(attrib_buttons == "guess_attrib_button") %>%
  group_by(subject, us_valence) %>%
  summarise(mean_eval_change = mean(eval_change_score)) %>% filter(n() > 1)

mod_guess_vma = aov_ez(dat_ev_vma_guess
                               ,id = "subject"
                               ,dv = "mean_eval_change"
                               ,within = "us_valence"
)

# mod_guess_vma_print = apa_print(mod_guess_vma)
# 
# apa_table(
#   mod_guess_vma_print$table
#   ,caption = "Repeated-measures ANOVA: Evaluative change scores as a function of US Valence for CSs that received a 'Guess' attribution in the Valence Memory Attribution task"
# )
# 
# knitr::kable(describeBy(dat_ev_vma_guess$mean_eval_change, dat_ev_vma_guess$us_valence, mat=TRUE), digits=2)

dat_ev_vma_guess_wide = dat_ev_vma_guess %>% pivot_wider(names_from = "us_valence"
                                                                         ,values_from = "mean_eval_change")

dat_ev_vma_guess_wide$diff_overall = dat_ev_vma_guess_wide$positive-dat_ev_vma_guess_wide$negative 

dat_ev_vma_guess_wide = dat_ev_vma_guess_wide %>% filter(!is.na(diff_overall))

bf_ev_guess = bfrr(
  sample_mean = mean(dat_ev_vma_guess_wide$diff_overall), # mean of the sample
  sample_se = sd(dat_ev_vma_guess_wide$diff_overall)/sqrt(length(dat_ev_vma_guess_wide$diff_overall)), # SE of the sample
  sample_df = length(dat_ev_vma_guess_wide$diff_overall) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = 23.5,
  tail = 1,
  criterion = 3, 
  rr_interval = list(
    # mean = c(-2, 2),
    sd = c(0, 500)
  )
)

#summary(bf_ev_guess)
```

```{r vma_eval_change_no_struct_k, include=FALSE}
#recode intuition and guessing attributions into "no_conscious_s_k"
dat_vma = dat_vma %>% mutate(recode_attrib = as.factor(ifelse(attrib_buttons=="memory_attrib_button", "conscious", "no_conscious_sk")))

#check whether it worked as intended
table(dat_vma$attrib_buttons, dat_vma$recode_attrib)
  
#for no conscious structural knowledge (intuition and guessing)
dat_ev_vma_no_conscious = dat_vma %>% filter(recode_attrib == "no_conscious_sk") %>%
  group_by(subject, us_valence) %>%
  summarise(mean_eval_change = mean(eval_change_score)) %>% filter(n() > 1)

mod_noc_vma = aov_ez(dat_ev_vma_no_conscious
                                ,id = "subject"
                                ,dv = "mean_eval_change"
                                ,within = "us_valence"
)

# mod_noc_vma_print = apa_print(mod_noc_vma)
# 
# apa_table(
#   mod_noc_vma_print$table
#   ,caption = "Repeated-measures ANOVA: Evaluative change scores as a function of US Valence for CSs that received an 'Intuition' or 'Guess' (no conscious structural knowledge) attribution in the Valence Memory Attribution task"
# )
# 
# knitr::kable(describeBy(dat_ev_vma_no_conscious$mean_eval_change, dat_ev_vma_no_conscious$us_valence, mat=TRUE), digits=2)

dat_ev_vma_no_conscious_wide = dat_ev_vma_no_conscious %>% pivot_wider(names_from = "us_valence"
                                                                           ,values_from = "mean_eval_change")

dat_ev_vma_no_conscious_wide$diff_overall = dat_ev_vma_no_conscious_wide$positive-dat_ev_vma_no_conscious_wide$negative 

dat_ev_vma_no_conscious_wide = dat_ev_vma_no_conscious_wide %>% filter(!is.na(diff_overall))

bf_ev_no_conscious = bfrr(
  sample_mean = mean(dat_ev_vma_no_conscious_wide$diff_overall), # mean of the sample
  sample_se = sd(dat_ev_vma_no_conscious_wide$diff_overall)/sqrt(length(dat_ev_vma_no_conscious_wide$diff_overall)), # SE of the sample
  sample_df = length(dat_ev_vma_no_conscious_wide$diff_overall) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = 23.5,
  tail = 1,
  criterion = 3, 
  rr_interval = list(
    # mean = c(-2, 2),
    sd = c(0, 500)
  )
)

#summary(bf_ev_no_conscious)
```

#### EC effect for intuition and guessing attributions?

We tested whether we replicate Waroquier et al.'s (2020) findings. The main results are displayed in Figure \@ref(fig:overall_vma_figure). Importantly, we were interested in whether we find an EC effect when participants reported no memory (random guessing).

Both frequentist and Bayesian analyses yielded evidence for an EC effect when participants made a memory attribution, `r apa_print(mod_memory_vma)$full_result`, $BF_{H(0, 23.5)}=$ `r bf_ev_memory$BF`. When participants made an intuition attribution, the EC effect was not significant, `r apa_print(mod_feeling_vma)$full_result`, and BF was inconclusive, $BF_{H(0, 23.5)}=$ `r bf_ev_feeling$BF`. 

Critically and contrary to Waroquier et al. (2020), we found no EC effect when participants made a random guessing attribution, `r apa_print(mod_guess_vma)$full_result`, and BF yielded evidence against an EC effect, $BF_{H(0, 23.5)}=$ `r bf_ev_guess$BF`. 

As done by Waroquier et al., we grouped intuition and random attributions into a single "no conscious structural knowledge" category. We did not find an EC effect for this category, `r apa_print(mod_noc_vma)$full_result`, and BF yielded evidence against it, $BF_{H(0, 23.5)}=$ `r bf_ev_no_conscious$BF`. 
