---
editor_options:
  chunk_output_type: console
output:
  pdf_document: default
  html_document: default
---

```{r prepare_variables, include=FALSE, echo = FALSE, warning=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, cache=TRUE)

library("papaja")
library(ggplot2)
library(lemon)
library(Matrix)
library(ggmosaic)

library(knitr)
library(afex)
options(contrasts=c('contr.sum', 'contr.poly'))
afex_options(check_contrasts=FALSE, method_mixed="S")

library(data.table)
library(lme4)
library(lmerTest)
library(tidyverse)
library(emmeans)

# Read and clean data set

# The CSV file contains the data from all 129 participants collected in Louvain by Adrian Mierop (using EPrime V2).
# It contains a lot of meaningless columns that are removed in a first step.

# read data for Exp.1 (taken from Adrien Mierop via Dropbox download)
d <- readr::read_delim("./data/modifiedPD.csv", ";", escape_double = FALSE, trim_ws = TRUE)

# remove meaningless columns
d$Group <- d$Block <- d$BlockList <- d$BlockList.Cycle <- d$BlockList.Sample <- d$`Running[Block]` <- d$conditioning.Cycle <-  NULL
d$`Procedure[SubTrial]` <- d$`Running[SubTrial]` <- d$IdentityMem <- d$IdentityMem.Cycle <- d$ListPair.Cycle <- d$PDB.Cycle <- NULL
d$postratings.Cycle <- d$preratings.Cycle <- d$ValenceMem.Cycle <-  NULL

# easy alias for important columns
d$task <- d$`Running[Trial]`
d$rating <- d$SlideSlider.RESP
d$vm.resp <- d$Slide3.RESP
d$vm.rt <- d$Slide3.RT
d$im.resp <- d$SlideIm1.RESP
d$im.rt <- d$SlideIm1.RT
d$pd.resp <- d$SlidePDB.RESP
d$pd.rt <- d$SlidePDB.RT
## Data structure

# The EPrime experiment consists of two *condition* branches, *inclusion* and *exclusionAttitude*.
# Variable *Subject* indexes participant number.
# 
# Within each branch, there are the following phases/procedures:
# 
# - *preratings*: all face CSs (variable *ICS*) are rated on a slider (variable *SlideSlider.RESP*)
# - ListPair (list construction assigns USs to selected critical CSs): *SC*, *typUS*, *US[1..16]*
# - *conditioning* (presentation phase): variables *ICS*, *typUS*, *US[Trial]*
# - *TrainingInclusion* (1 trial for each of 8 cases): *correct*, *Slide2.RESP*, *Slide2.RT*
# - *PDB* (main PD procedure): variables *SC*/*CS* (paired CS: SC; filler CS: CSneg, CSpos), *SlidePDB.RESP*, *SlidePDB.RT* 
# - *postratings*: see *preratings*
# - *ValenceMem*: variables *ICS*, *Slide3.RESP*, *Slide3.RT* (1=pleasant, 2=unpleasant, 3=*don't know*)
# - *IdentityMem*: variables *ICS*, *SlideIm1.RESP*, *SlideIm1.RT*, *idMemSel* (selected US needs to be checked against ListPair data)


# Prepare variables

# exclude practice-run Subject 1
d <- subset(d, Subject>1)
# had problems with CS/US assignment
d <- subset(d, Subject != 67)
d <- subset(d, Subject != 71)
d <- subset(d, Subject != 60)
d <- subset(d, Subject != 78)
# was run in both inclusion and exclusion conditions
d <- subset(d, Subject != 87) 

## for a given CS, read out the paired USs
list <- subset(d, task=="ListPair", select=c(Condition, Subject, SC, typUS, `US[SubTrial]`))
names(list) <- c("Condition","Subject","ICS","typUS","US")
lu <- unique(subset(list, select=-US))

## evaluative ratings
pre <- subset(d, task=="preratings", select = c(Condition, Subject, ICS, rating))
names(pre) <- c("Condition","Subject","ICS","prerating")
post <- subset(d, task=="postratings", select = c(Condition, Subject, ICS, rating))
names(post) <- c("Condition","Subject","ICS","postrating")

## memory for US valence and US identity
vm <- subset(d, task=="ValenceMem", select = c(Condition, Subject, ICS, vm.resp, vm.rt))
im <- subset(d, task=="IdentityMem", select = c(Condition, Subject, ICS, im.resp, im.rt, US1, US2, US3, US4, US5, US6, US7, US8, US9, US10, US11, US12, US13, US14, US15, US16, USi1, USi2, USi3, USi4, USi5,USi6, USi7, USi8, USi9, USi10, USi11, USi12, USi13, USi14, USi15, USi16))

### compute IDmem correctness

l <- merge(im, list, by=c("Condition","Subject", "ICS"), all=FALSE)
l[6:21] <- lapply(l[6:21], function(x) substr(x,7, 11))
l[,"US"] <- NULL
l <- unique(l)

l$selec <- paste('USi', l$im.resp, sep='')

l <- data.table(l)

l[selec %in% "USi1", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi2", c("USi1", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi3", c("USi2", "USi1", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi4", c("USi2", "USi3", "USi1", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi5", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi6", c("USi2", "USi3", "USi4", "USi5", "USi1", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi7", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi1", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi8", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi1", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi9", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi1", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi10", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi1", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi11", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi1", "USi12", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi12", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi1", "USi13", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi13", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi1", 'USi14', "USi15", "USi16")  := NA ]
l[selec %in% "USi14", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi1', "USi15", "USi16")  := NA ]
l[selec %in% "USi15", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi1", "USi16")  := NA ]
l[selec %in% "USi16", c("USi2", "USi3", "USi4", "USi5", "USi6", "USi7", "USi8", "USi9", "USi10", "USi11", "USi12", "USi13", 'USi14', "USi15", "USi1")  := NA ]
l[, selUS := sum(USi1, USi2, USi3, USi4, USi5, USi6, USi7, USi8, USi9, USi10, USi11, USi12, USi13, USi14, USi15, USi16, na.rm= TRUE), 1:nrow(l)]
l[selUS == US1 | selUS == US2| selUS == US3| selUS == US4| selUS == US5| selUS == US6| selUS == US7| selUS == US8, im.correct := 1]
l[selUS == US9 | selUS == US10| selUS == US11| selUS == US12| selUS == US13| selUS == US14| selUS == US15| selUS == US16, im.correct := 0]
lc <- aggregate(x=l$im.correct, by=list(Condition=l$Condition, Subject=l$Subject, ICS=l$ICS), FUN=any)
names(lc) <- c("Condition","Subject","ICS","im.correct")
im <- merge(subset(im, select=c(Condition, Subject, ICS, im.resp, im.rt)), lc, by=c("Condition","Subject","ICS"))


## pd training accuracy
pdtrain <- subset(d, !is.na(correct), select = c(Condition, Subject, TrainingInclusion.Sample, correct, Slide2.RESP, Slide2.RT))
pdtrain$ACC <- ifelse(pdtrain$correct == pdtrain$Slide2.RESP, 1, 0)

#write.csv(pdtrain, file="./data/modPD1_pdtrain.csv")

pdc <- aggregate(pdtrain$ACC, by=list(Condition=pdtrain$Condition, Subject=pdtrain$Subject), FUN=mean)
names(pdc) <- c("Condition","Subject","pdtrain.correct")


## pd responses
pd <- subset(d, task=="PDB", select=c(Condition, Subject, ICS, CS, SC, CSpos, CSneg, pd.resp, pd.rt))
pd$typCS <- ""
pd$typCS[pd$ICS == pd$SC] <- "neutral"
pd$typCS[pd$ICS == pd$CS & !is.na(pd$CSpos)] <- "pos"
pd$typCS[pd$ICS == pd$CS & !is.na(pd$CSneg)] <- "neg"

## merge variables
dd <- merge(pre, post, by=c("Condition","Subject","ICS"))
dd <- merge(dd, lu, by=c("Condition","Subject","ICS"))
dd <- merge(dd, pdc, by=c("Condition","Subject"))
dd <- merge(dd, subset(pd, select=c(Condition, Subject, ICS, typCS,pd.resp, pd.rt)), by=c("Condition","Subject","ICS"))
dd <- merge(dd, vm, by=c("Condition","Subject","ICS"))
dd <- merge(dd, im, by=c("Condition","Subject","ICS"))

### compute valmem correctness
dd$vm.resptype <- "dont know"
dd$vm.resptype[dd$vm.resp==1 & dd$typUS=="pos"] <- "correct"
dd$vm.resptype[dd$vm.resp==2 & dd$typUS=="neg"] <- "correct"

dd$vm.resptype[dd$vm.resp==1 & dd$typUS=="neg"] <- "incorrect"
dd$vm.resptype[dd$vm.resp==2 & dd$typUS=="pos"] <- "incorrect"

dd$vm.correct <- NA
dd$vm.correct[dd$vm.resp==1 & dd$typUS=="pos"] <- TRUE
dd$vm.correct[dd$vm.resp==2 & dd$typUS=="neg"] <- TRUE
dd$vm.correct[dd$vm.resp==1 & dd$typUS=="neg"] <- FALSE
dd$vm.correct[dd$vm.resp==2 & dd$typUS=="pos"] <- FALSE

# exclude Subject 1
dd <- subset(dd, Subject>1)

## create factors
dd$ec <- dd$postrating - dd$prerating 
dd$Condition <- relevel(factor(dd$Condition, labels=c("Exclusion", "Inclusion")), ref="Inclusion")
dd$Subject <- as.factor(dd$Subject)
dd$ICS <- as.factor(dd$ICS)
dd$typCS <- as.factor(dd$typCS)
dd$typUS <- as.factor(dd$typUS)
dd$pd.resp <- as.factor(dd$pd.resp)
dd$vm.resp <- factor(dd$vm.resp, labels=c("pleasant", "unpleasant", "dont know")) # (1=pleasant, 2=unpleasant, 3=*don't know*)
dd$im.resp <- as.factor(dd$im.resp)
dd$vm.resptype <- as.factor(dd$vm.resptype)
dd$imcor <- as.numeric(dd$im.correct)
dd$vmcor <- as.numeric(dd$vm.correct)

## PD responses

dd$pdset <- NA
dd$pdset[dd$pd.resp %in% c(1,2)] <- 1
dd$pdset[dd$pd.resp %in% c(3,4)] <- 2
dd$pdset <- factor(dd$pdset, labels = c("Memory", "Attitude"))

dd$pdval <- NA
dd$pdval[dd$pd.resp %in% c(1,3)] <- 1
dd$pdval[dd$pd.resp %in% c(2,4)] <- 2
dd$pdval <- factor(dd$pdval, labels=c("pleasant", "unpleasant"))

dd$pdcor <- FALSE
dd$pdcor[dd$pdval == "pleasant" & dd$typUS == "pos"] <- TRUE
dd$pdcor[dd$pdval == "unpleasant" & dd$typUS == "neg"] <- TRUE
dd$pdcor_ <- as.numeric(dd$pdcor)

## median-split of pdtrain.correct
dd$pdtrain.split <- ifelse(dd$pdtrain.correct>.75,1,2)
pdq <- quantile(dd$pdtrain.correct, probs=c(.33,.66), na.rm=TRUE)
#table(dd$pdtrain.split, dd$pdtrain.correct)

## triple-split: below .5 (discard) vs. 2 above 
dd$pdtrain.split3 <- ifelse(dd$pdtrain.correct>=pdq[2],1,ifelse(dd$pdtrain.correct>.5,2,3))
#table(dd$pdtrain.split3, dd$pdtrain.correct)
#table(dd$pdtrain.split3)
dd$pdtrain.split3 <- as.factor(dd$pdtrain.split3)



## prepare variable and level labels for reporting

dd$typCS_[dd$typCS=="neutral"] <- "paired" 
dd$typCS_[dd$typCS=="neg" | dd$typCS=="pos"] <- "nonpaired"
dd$pdset_ <- as.numeric(dd$pdset)
dd$`Proportion of Memory Responses` <- as.numeric(dd$pdset_ == 1)
dd$`Pairing` <- dd$typCS_
dd$`Proportion pleasant` <- as.numeric(dd$pdval == "pleasant")
dd$`US Valence` <- dd$typUS
dd$`PD Instruction` <- dd$Condition
dd$`Proportion correct` <- dd$pdcor_
dd$`PD Response` <- factor(dd$pdset, labels = c("Memory", "Attitude"))
dd$`CS Type` <- factor(dd$typCS, labels=c("negative","neutral","positive"))
dd$Response <- dd$vm.resptype
dd$Valence <- dd$`US Valence`


dd$ecmag <- dd$ec
dd$ecmag[dd$typUS=="neg"] <- -dd$ec[dd$typUS=="neg"]

## computing more variables

d1 <- dd

d1$`Proportion consistent` <- d1$`Proportion correct`
d1$`Proportion consistent`[d1$Condition=="Exclusion"] <- 1-d1$`Proportion correct`[d1$Condition=="Exclusion"]

d1$`Proportion dont know` <- d1$vm.resp=="dont know"
d1$`Valence memory accuracy` <- factor(d1$vmcor, labels=c("incorrect","correct"))
#d1$pdval_ <- d1$pdval-1
d1$vm.pleasant <- ifelse(d1$vm.resp == "pleasant",1,0)
d1$vm.pleasant[d1$vm.resptype=="dont know"] <- NA
d1$dk <- as.numeric(d1$Response=="dont know")
d1$acc <- as.numeric(d1$Response=="correct")

## store
#write.csv(d1, file="./data/modPD1_clean.csv")
save(d1, file="./data/modPD1_d1.RData")

#table(d1$pdtrain.split3)
#length(unique(d1$Subject[d1$pdtrain.split3==2]))

d1p <- subset(d1, Pairing=="paired")
d1np <- subset(d1, Pairing=="nonpaired")
d1pd <- subset(d1, pdtrain.split3 != 3)
d1.neutral <- subset(d1, typCS=="neutral")
d1.paired.Mem <- subset(d1, pdset=="Memory" & Pairing=="paired")
d1.nonpaired.Att <- subset(d1, pdset=="Attitude" & Pairing=="nonpaired")
d1.In <- subset(d1, Condition =="Inclusion")

# sanity checks

## plot design variables to check balancing
# plot(dd[,c(1,2,3,6,8)])
# - Subject numbers confounded with Condition: blockwise assignment?

## plot dvs 
#plot(dd[,c(4,5,15,17)])
# pre & post are correlated
# mem accuracy does not appear to depend on valence/ratings

# needed below: d, dd, pdc


#read.csv(dd, file="modPD1_clean.csv")

```


## Results and Discussion

This section is divided into three parts:
First, we report results of the memory and attitude manipulations to address the validity of the modified procedure.
Second, we assess the accuracy of introspection;
third, we assessed the invariance assumption.
Full results on the effects of the experimental factors on evaluative ratings and valence-memory measures are reported in the Appendix.
^[We used `r cite_r("r-references.bib")` for all our analyses.]

To gauge data quality, participants' understanding of PD instructions was assessed before they engaged in the task.
Overall, mean accuracy was approximately 75%.
<!-- (i.e., participants responded correctly in 6 out of 8 cases). -->
To assess whether understanding of PD instructions affected the results, we split the sample into three subgroups according to the PD Training Accuracy variable.
^[We separated participants at the median and the .5 accuracy level, thus creating high-, medium-, and low-accuracy subgroups (note that, as accuracy values could take on only 8 different values, equally sized subgroups could not be created; so we opted instead for meaningful cutoffs).]
<!-- (11 from Inclusion & 14 from the Exclusion condition).]  -->
Most results were unaffected by this variable; below we report all exceptions.


### Validity of the modified PD procedure

To assess whether the modified PD procedure is a valid measure of subjective memory for CS-US pairings and of attitudes, we investigated whether (1) pairing affects participants' subjective memory experience (which we expected to differ between paired versus nonpaired control CSs) as reflected in the frequency of Memory button responses; and (2) participants' pre-rated attitudes are consistently reflected in the Attitude button responses.

#### Proportion of Memory responses

```{r response-set, fig.cap="Left: Memory responses for paired vs. nonpaired CSs; right: pre-rating consistent Attitude responses for nonpaired stimuli."}

par(mfrow=c(1,2))
par(cex = 1.0)

## data subset for testing selective influence & introspective accuracy

pdset_aov <- aov_ez(data=d1, id="Subject", within=c("Pairing", "Valence"), between=c("Condition","pdtrain.split3"), dv="Proportion of Memory Responses", fun_aggregate = mean)
pdset_out <- apa_print(pdset_aov)
em_Pairing <- apa_print(emmeans::emmeans(pdset_aov, ~Pairing))

tt <- table(d1.In$pdset, d1.In$typCS_)
#ftable(tt)
pt <- prop.table(tt, 2)
rownames(pt) <- c("Set1","Set2")
#kable(pt, caption="Response button set as a function of CS type", digits = 2, row.names = TRUE)

pdset_aov_p <- aov_ez(data=d1p, id="Subject", within=c("Valence"), between=c("Condition", "pdtrain.split3"), dv="Proportion of Memory Responses", fun_aggregate = mean)
em_Cond_ <- emmeans::emmeans(pdset_aov_p, ~ `Condition`)
em_Cond <- apa_print(em_Cond_)
em_Cond_c <- apa_print(pairs(emmeans::emmeans(pdset_aov_p, ~`Condition`)))

#papaja::apa_lineplot(data=d1p, id="Subject", factors=c("Condition", "Valence"), dv="Proportion of Memory Responses", ylim=c(.5,1), args_legend=list(x="topleft"))

papaja::apa_lineplot(data=d1, id="Subject", factors=c("PD Instruction", "Pairing"), dv="Proportion of Memory Responses", ylim=c(0,1), args_legend=list(x="topleft"))


pdset_aov_np <- aov_ez(data=d1np, id="Subject", within=c("Valence"), between=c("PD Instruction","pdtrain.split3"), dv="Proportion of Memory Responses", fun_aggregate = mean)
#papaja::apa_barplot(data=d1np, id="Subject", factors=c("pdtrain.split3", "Valence"), dv="Proportion of Memory Responses", args_legend=list(x="topleft"))
em_Cond_np_ <- emmeans::emmeans(pdset_aov_np, ~ `Valence`)

## plot from next chunk
d1.nonpaired.Att$`PD Training Accuracy` <- as.factor(d1.nonpaired.Att$pdtrain.split3) 
papaja::apa_lineplot(data=d1.nonpaired.Att, dv="Proportion consistent", id="Subject", factors=c("PD Instruction","PD Training Accuracy"), args_legend=list(x="bottomleft"), ylim=c(.5,1), intercept=.5 )


```

To assess the validity of the modified procedure, we first analyzed the proportion of Memory (rather than Attitude) responses in a 2 (Pairing) x 2 (Valence) x 2 (PD Instruction) x 3 (PD Training Accuracy) ANOVA to check whether it reflected memory for pairings.
Memory responses (Figure \@ref(fig:response-set), left) were more frequent for paired CSs, `r em_Pairing$estimate$paired`, than for valent control stimuli (that were never paired with any US), `r em_Pairing$estimate$nonpaired`, `r pdset_out$full_result$Pairing`.

Additional (much smaller) effects of Valence, `r pdset_out$full_result$Valence`, PD Training Accuracy, `r pdset_out$full_result$pdtrain_split3`, and PD Instruction, `r pdset_out$full_result$Condition` were found.
The PD Instruction was restricted to the subset of paired CSs, `r apa_print(pdset_aov_p)$full_result$Condition` (with $F<1$ for nonpaired stimuli): 
The proportion of Memory responses was greater under Exclusion than Inclusion instructions (see below).
The other two effects were restricted to nonpaired stimuli (for paired CSs, largest $F=1.62$, smallest $p=.21$):
Valence affected Memory responses for these items, `r apa_print(pdset_aov_np)$full_result$Valence`, with more Memory responses for positive ($M=.1$, 95% CI [.07, .12]) than negative ($M=.07$, 95% CI [.04, .09]) stimuli;
and a main effect of PD Training Accuracy, `r apa_print(pdset_aov_np)$full_result$pdtrain_split3`, reflected more Memory responses for the least accurate third of participants (the effect disappeared after these participants were removed).


#### Attitude responses as a function of pre-existing attitudes


```{r pd2-acc, fig.cap="Accuracy of PD Memory responses to paired CSs (left) and of Attitude responses to valent unpaired stimuli (right) as a function of Valence and PD Instruction"}

par(mfrow=c(1,2))
par(cex = 1.2)

# Attitude responses for unpaired valent CSs

pdset2_aov <- aov_ez(data=d1.nonpaired.Att, id="Subject", within=c("Valence"), between=c("Condition", "pdtrain.split3"), dv="Proportion consistent", fun_aggregate = mean) 
## Cond, pdtrain, cond:pdtrain
pdset2_out <- apa_print(pdset2_aov)

em2_Condition_ <- emmeans(pdset2_aov, ~Condition)
em2_Condition_c <- apa_print(pairs(em2_Condition_))
em2_Condition <- apa_print(em2_Condition_)

```

Next, we computed a 2 (Valence) x 2 (PD Instruction) x 3 (PD Training Accuracy) ANOVA to test the influence of the attitude manipulation (i.e., positively vs. negatively pre-rated CSs) on the Attitude responses to nonpaired valent stimuli.
We expected participants' Attitude responses to be consistent with their pre-rated evaluations.

The right panel of Figure \@ref(fig:response-set) shows the accuracy (i.e., consistency with pre-rated evaluation) of Attitude responses: 
Overall, the Attitude responses to non-paired valent stimuli accurately matched their initial pre-rated evaluations.
Accuracy was affected by PD Instruction, `r pdset2_out$full_result$Condition`: 
Responses were less accurate under Exclusion (`r em2_Condition$estimate$Exclusion`) than Inclusion instructions (`r em2_Condition$estimate$Inclusion`), suggesting that the reversal of the response mapping required under Exclusion failed in a substantial percentage of cases.
In addition, a main effect of PD Training Accuracy was obtained, `r pdset2_out$full_result$pdtrain_split3`, as well as an interaction with PD Instruction, `r pdset2_out$full_result$Condition_pdtrain_split3`:
The PD Instruction effect was limited to the subset of participants who did worst on the PD Training test; it was not found for the other subgroups.


```{r modval1, fig.cap="Proportion of 'pleasant' responses as a function of CS Type and Response Set (left: Memory; right: Attitude)"}

par(mfrow=c(1,2))

pdset1pair <- aov_ez(data=subset(d1pd, pdset=="Memory" & typCS_=="paired"), id="Subject", dv="Proportion pleasant", within=c("Valence"), between=c("PD Instruction"))
# only Valence main effect

pdset1nonp <- aov_ez(data=subset(d1pd, pdset=="Memory" & typCS_=="nonpaired"), id="Subject", dv="Proportion pleasant", within=c("Valence"), between=c("PD Instruction"))
# too many exclusions, so Valence effect F(1,13)=3.08 p=.1

pdset2nonp <- aov_ez(data=subset(d1pd, pdset=="Attitude" & typCS_=="nonpaired"), id="Subject", dv="Proportion pleasant", within=c("Valence"), between=c("PD Instruction"))
# Valence; Condition:Valence

pdset2pair <- aov_ez(data=subset(d1pd, pdset=="Attitude" & typCS_=="paired"), id="Subject", dv="Proportion pleasant", within=c("Valence"), between=c("PD Instruction"))
# nix

pdset2_ec <- aov_ez(data=subset(d1pd, pdset=="Attitude" & `PD Instruction`=="Inclusion" & `CS Type`=="neutral"), id="Subject", dv="Proportion pleasant", within=c("US Valence"))
pdset2_ec_out <- apa_print(pdset2_ec)

```

These findings support the validity of the modified procedure in a manner 
parallel to the validation of the original PD procedure:
Participants' attitudes toward clearly valenced stimuli are consistently reflected in the proposed attitude measure (i.e., 96% consistency); and participants' subjective memory for pairings is affected by the memory manipulation as expected.
At this point, the support for the modified procedure can be considered comparable to that of the original PD paradigm.
<!-- Next, we checked whether the core assumption of perfect introspection underlying the original PD approach was corroborated. -->
The following analyses test assumptions underlying the original PD paradigm.

### Testing assumptions: Introspective accuracy

The perfect-introspection assumption predicts that participants use the Memory responses only for those CSs for which they actually have memory (i.e., not for unpaired CSs).
Also, participants should have perfect US valence memory for those CSs for which they used the Memory responses, and they should have no US valence memory for those CSs for which they used the Attitude responses.

#### "Remembered" US valence for unpaired stimuli

In the previous section (Fig. \@ref(fig:response-set) left), we found that the first prediction was contradicted:
Despite the fact that there could be no veridical US valence memory for unpaired CSs, participants indicated "remembering" US valence in approximately 8% of these cases.
In other words, they reported introspective *remember* decisions in the absence of veridical US memory (i.e., introspective false alarms). 

#### "Remembered" US valence for paired stimuli

```{r pd-main}

d1.neutral.Mem <- subset(d1, typCS=="neutral" & pdset=="Memory")
d1.neutral.Att <- subset(d1, typCS=="neutral" & pdset=="Attitude")


## plot
#papaja::apa_barplot(data=subset(dd_, typCS=="neutral"), id="Subject", dv="pdcor", factors=c("pdset", "Condition", "pdtrain.split3"), intercept=.5)

## anovas
pdset1_aov <- apa_print(aov_ez(data=d1.neutral.Mem, id="Subject", dv="pdcor_",  between=c("PD Instruction", "pdtrain.split3"))) 
## no effects
pdset2_aov <- apa_print(aov_ez(data=d1.neutral.Att, id="Subject", dv="pdcor_", between=c("PD Instruction", "pdtrain.split3"))) 
## effect of pdtrain.split3: PD becomes less accurate with more errors on training


```


```{r pd1-acc, fig.cap="Accuracy of PD Memory responses to paired CSs (left) and of Attitude responses to valent unpaired stimuli (right) as a function of Valence and PD Instruction."}

par(mfrow=c(1,2))
par(cex = 1.2)

# Memory responses for paired CSs

pdset1_aov <- aov_ez(data=d1.paired.Mem, id="Subject", within=c("Valence"), between=c("PD Instruction", "pdtrain.split3"), dv="Proportion correct", fun_aggregate = mean)
em2_Condition_ <- emmeans(pdset1_aov, "1")
pdset1_out <- apa_print(pdset1_aov)


```

To test the second prediction, we assessed accuracy of Memory responses for paired CSs (i.e., whether the indicated valence corresponds to the US's actual valence).
A 2 (PD Instruction) x 2 (Valence) x 3 (PD Training Accuracy) ANOVA of the accuracy of Memory responses for paired CSs obtained no significant effects (only a descriptive advantage for CSs paired with negative USs, $p = .05$).
Only 80% of responses were correct (95% CI [77, 83]).
^[Assuming a .5 guessing tendency in the absence of memory, the 20% error rate suggests that, of these 80%, another 20 percentage points represent lucky guesses, so that only the remaining 60 percentage points represent veridical memory.]
This far-from-perfect level of US memory accuracy for Memory responses again indicates introspective false alarms.


#### Objective US valence memory for "non-remembered" US valence

Another test of the introspective-accuracy prediction concerns the Attitude responses for paired CSs (i.e., cases for which, subjectively, no memory is detected):
Do these cases indeed reflect a complete lack of memory?
Because the PD procedure does not assess objective memory for Attitude responses, we used the subsequent US valence and US identity memory measures to assess memory for Attitude responses to paired CSs.

```{r pd-usmem2, fig.cap="Left: Proportion of valence-memory 'dont know' responses for neutral CSs; right: Accuracy of valence-memory responses."}

par(mfrow=c(1,2))
par(cex=1.0)


## analyze proportion "dontknow" as index for criterion

a.vm.dk <- afex::aov_ez(data = d1.neutral, id="Subject", within=c("pdset"), between=c("Condition", "pdtrain.split3"), dv="dk")
#papaja::apa_barplot(a.vm.dk, ylim=c(0,1), args_legend=list(x="topleft"))
d1.neutral$`Proportion dont know` <- d1.neutral$dk
papaja::apa_lineplot(data=d1.neutral, dv="Proportion dont know", id="Subject", factors=c("PD Response","PD Instruction"), ylim=c(0,1), args_legend=list(x="topleft"))


## identity memory

imcor <- afex::aov_ez(data = d1.neutral, id="Subject", within=c("pdset"), between=c("PD Instruction","pdtrain.split3"), dv="imcor")
#papaja::apa_barplot(imcor, ylim=c(0,1), args_legend=list(legend=NULL), intercept=.5)

usidmem <- emmeans(imcor, "1")

## analyze proportion correct as index for accuracy

a.vm.acc <- afex::aov_ez(data = subset(d1.neutral, Response!="dont know"), id="Subject", within=c("pdset"), between=c("Condition", "pdtrain.split3"), dv="acc")
#papaja::apa_barplot(a.vm.acc, ylim=c(0,1), args_legend=list(legend=NULL), intercept=.5)
d1.neutral$`Valence memory Accuracy` <- d1.neutral$acc
papaja::apa_lineplot(data=subset(d1.neutral, Response!="dont know"), dv="Valence memory Accuracy", id="Subject", factors=c("PD Response","PD Instruction"), ylim=c(0,1), args_legend=list(plot=FALSE), intercept=.5)

## is valmem above chance for Set2 ?

dd_above5 <- subset(d1.neutral, pdset=="Attitude" & Response!="dont know")
dd5_ <- aggregate(dd_above5$acc, by=list(dd_above5$Subject, dd_above5$`PD Instruction`), FUN=mean)
above5 <- apa_print(t.test(x=dd5_$x, mu=.5))
above5_ie <- apa_print(t.test(x~Group.2, data=dd5_))

```

We computed a 2 (PD Response) x 2 (PD Instruction) x 3 (PD Training Accuracy) ANOVA of US identity memory accuracy.
Memory for US identity was unexpectedly found to be at chance in this study (*M*=.51, 95% *CI*: .49, .54; no significant effects were obtained, smallest *p*=.09).
In retrospect, this finding is plausible considering the pairing schedule:
Any US was paired only once with a given CS, implying weakly encoded CS-US episodes; USs were paired with multiple CSs, and CSs were paired with 8 different USs, resulting in many partially overlapping, potentially interfering episodes.

For the valence memory measure, we computed two separate 2 (PD Response) x 2 (PD Instruction) x 3 (PD Training Accuracy) ANOVAs of accuracy and the proportion of "don't know" responses.
Figure \@ref(fig:pd-usmem2) (left) shows the proportion of "don't know" responses which reflect participants' uncertainty.
It can be seen that uncertainty is higher for Attitude (versus Memory) responses, `r apa_print(a.vm.dk)$full_result$pdset`, suggesting that memory is indeed lower for Attitude as compared to Memory responses.
Uncertainty is further increased for the Inclusion (versus Exclusion) groups, `r apa_print(a.vm.dk)$full_result$Condition`, suggesting a violation of invariance in a carry-over effect of PD Instructions on the subsequent valence-memory task.
The significant interaction reflected the finding that the uncertainty reduction for the Exclusion group is larger for Attitude than Memory responses, `r apa_print(a.vm.dk)$full_result$Condition_pdset`.

US valence memory accuracy (Figure \@ref(fig:pd-usmem2), right panel) was higher in Memory than in Attitude responses, `r apa_print(a.vm.acc)$full_result$pdset`, but did not differ between PD Instructions, `r apa_print(a.vm.acc)$full_result$Condition` (the interaction was not significant, `r apa_print(a.vm.acc)$full_result$Condition_pdset`).
Importantly, substantially above-chance US valence memory accuracy was present even for those CSs for which Attitude responses were used in the PD task, `r above5$full_result`.
Such cases of Attitude responses with objective memory represent critical introspective misses. 
^[This finding is unlikely to be based on attitude-as-information heuristics given that the stimuli were individually selected to be initially neutral for a given participant.]
In other words, participants used the Attitude responses in a way that is incompatible with the perfect-introspection assumptions underlying the PD approach.

Overall, the data contradict the predictions derived from the perfect-introspection assumption, and they support our notion that introspective misses and false alarms occur in the PD procedure.


### Testing assumptions: Invariance

This section addresses three ways in which the invariance assumption (i.e., of identical cognitive processes in both inclusion and exclusion conditions) may be violated:
First, we investigate the accuracy of participants' understanding of the instructions.
Next, we test whether PD Instruction affected subjective memory decisions.
Finally, we analyze PD performance (i.e., RT and accuracy of PD responses) to probe for a higher difficulty of the exclusion task.

#### Understanding of PD instructions 


```{r pd-training, fig.cap="Accuracy in PD training phase as a function of PD Condition (left) and scenario (right)."}

par(cex=1.0)

#pdtrain <- read.csv("./data/modPD1_pdtrain.csv")

# describe and exclude

# hist(dd$pdtrain.correct)
# table(dd$pdtrain.correct, dd$pdtrain.split3)
#table(dd$Condition, dd$pdtrain.split3)
# inclusion: 11 Ss
# exclusion: 14 Ss

# analyze

## acc

understandACC_ <- aov_ez(data=subset(pdtrain), id="Subject", dv="ACC", within=c("TrainingInclusion.Sample"), between=c("Condition"))

## RT

understandRT_ <- aov_ez(data=subset(pdtrain), id="Subject", dv="Slide2.RT", within=c("TrainingInclusion.Sample"), between=c("Condition"))

# plot

par(mfrow=c(1,2))

#papaja::apa_lineplot(data=pdtrain, id="Subject", dv="ACC", factors=c("Condition"))

#papaja::apa_beeplot(data=subset(dd), id="Subject", dv="pdtrain.correct", factors=c("Condition"))

#papaja::apa_lineplot(data=subset(dd, pdtrain.split3!=3), id="Subject", dv="pd.rt", factors=c("Condition"))

#papaja::apa_lineplot(data=pdtrain, id="Subject", dv="Slide2.RT", factors=c("Condition"))
#papaja::apa_beeplot(data=pdtrain, id="Subject", dv="Slide2.RT", factors=c("Condition"))

# invariance

understandACC <- apa_print(t.test(ACC~Condition, data=pdtrain))
understandRT <- apa_print(t.test(Slide2.RT~`Condition`, data=pdtrain))


#papaja::apa_barplot(data=pdtrain, id="Subject", dv="ACC", factors=c("TrainingInclusion.Sample","Condition"))

understand_ <- apa_print(aov_ez(data=subset(pdtrain, Subject!=87), id="Subject", dv="ACC", within=c("TrainingInclusion.Sample"), between=c("Condition")))
## effect of case, no interaction with condition

```


We analyzed participants' accuracy on the test of instruction understanding, which was administered after the PD instructions.
Previous research has suggested that the PD exclusion instructions are particularly difficult to follow (i.e., some participants fail to understand them even after repeated instruction-test cycles), so much so that they interfere with subsequent responding under inclusion instructions if manipulated within-subjects.
If difficulty differs between Inclusion and Exclusion, this may prove particularly problematic for the original PD approach.
We assessed whether participants understand both instructions equally well (i.e., whether they readily implement them in the test scenarios).
We compared the number of participants that had to be excluded from each group; the accuracy of instruction understanding in each group; and RT in the test of instruction understanding.
However, neither measure showed significant effects of PD Instruction: 
The test of instruction understanding did not yield evidence for a higher difficulty of Exclusion instructions.

#### Proportion of Memory responses under Inclusion versus Exclusion

The proportion of Memory responses is depicted in Figure \@ref(fig:response-set) (left panel).
We have seen that among paired CSs there were more Memory responses under Exclusion (`r em_Cond$estimate$Exclusion`) than Inclusion (`r em_Cond$estimate$Inclusion`), `r em_Cond_c$full_result$Inclusion_Exclusion`.
Thus, in violation of the invariance assumption, we found that the proportion of subjective *remember* decisions was affected by PD Instruction.
Participants in the Exclusion group may have tended to select the Memory response in order to avoid the more complex reversed response mapping required for an Attitude response. 

#### Difficulty of Inclusion versus Exclusion


```{r pd-rt, fig.cap="RT of PD responses as a function of PD Condition and PD Response (left: Exp.1; right: Exp.2)."}

par(cex=1.0)

# exclude extremely long RTs

cutoff <- fivenum(d1$pd.rt)[4]+3*IQR(d1$pd.rt)
d1.rt <- subset(d1, pd.rt<=cutoff)

#a.pdrt <- afex::aov_ez(data = subset(d1.rt, typCS=="neutral" & pdtrain.split3!=3), id="Subject", between=c("Condition"), within=c("pdset"), dv="pd.rt")
a.pdrt_ <- afex::aov_ez(data = subset(d1.rt), id="Subject", between=c("PD Instruction", "pdtrain.split3"), within=c("pdset"), dv="pd.rt")
## Condition:pdset: longer in Exclusion for Attitude responses
#papaja::apa_lineplot(data = subset(d1.rt), id="Subject", factors=c("Response Button Set","Condition"), dv="pd.rt", args_legend=list(x="topleft"))
#papaja::apa_lineplot(data = subset(d1.rt), id="Subject", factors=c("Response Button Set","Condition","Pairing"), dv="pd.rt", args_legend=list(x="topleft"))


# paired: two listwise exclusions

a.pdrt_p <- afex::aov_ez(data = subset(d1.rt, Pairing=="paired"), id="Subject", between=c("Condition","pdtrain.split3"), within=c("pdset"), dv="pd.rt")
## pdtrain (more accurate subjects are slower), pdset (Att slower), condition:pdset (Att slower esp for Exclusion)
#papaja::apa_lineplot(data = subset(d1.rt, Pairing=="paired"), id="Subject", factors=c("Response Button Set","Condition"), dv="pd.rt", args_legend=list(x="topleft"))


# nonpaired: many exclusions

a.pdrt_np <- afex::aov_ez(data = subset(d1.rt, Pairing=="nonpaired"), id="Subject", between=c("Condition","pdtrain.split3"), within=c("pdset"), dv="pd.rt")
##
d1.rt$pdtrain <- factor(d1.rt$pdtrain.split3)
a.pdrt_np_ <- afex::mixed(data = subset(d1.rt, Pairing=="nonpaired"), pd.rt~pdset*Condition*pdtrain+(pdset|Subject), method="S", progress=FALSE)
#papaja::apa_lineplot(data = subset(d1.rt, Pairing=="nonpaired"), id="Subject", factors=c("Response Button Set","Condition"), dv="pd.rt", args_legend=list(x="topleft"), use="all.obs")

## nonpaired: main effect of pdset (slower responses for the few erroneous Memory responses)

## (overall RTs tend to be longer under Exclusion)
## RTs longer for Set 2 ('attitude' responses)
## this is especially so under Exclusion


```

@dehouwer_differences_1997 suggested that one of the possible reasons for an invariance violation is that participants invest different amounts of effort into memory retrieval under Inclusion versus Exclusion instructions.
When response latency was used as an index of retrieval effort, he found that instruction differences indeed affected retrieval effort.
After excluding extreme RTs (i.e., those more than 3 interquartile ranges above the third quartile), we ran a 2 (PD Instruction) x 2 (PD Response) x 3 (PD Training Accuracy) ANOVA on the paired CSs to test whether such response latency differences were present here as well.
<!-- RT was found to be greater under Exclusion than Inclusion, `r apa_print(a.pdrt_p)$full_result$Condition`. -->
<!-- Figure \@ref(fig:pd-rt) illustrates this pattern. -->
RT was found to be greater for Attitude responses, `r apa_print(a.pdrt_p)$full_result$pdset`.
The significant interaction with PD Instruction, `r apa_print(a.pdrt_p)$full_result$Condition_pdset`, reflects the finding that Attitude responses take longer under Exclusion than Inclusion instructions.
^[An additional main effect of PD Training Accuracy, `r apa_print(a.pdrt_p)$full_result$pdtrain_split3`, reflected the finding that participants who were more accurate on the PD Training test also took more time to respond.]

The finding of longer Exclusion RT for Attitude responses is consistent with the above result (see Figure \@ref(fig:response-set), right panel) that Attitude responses were less accurate under Exclusion instructions (i.e., Attitude judgments of valent nonpaired stimuli were less likely to match pre-rated evaluations) for the subgroup of participants who performed least accurately on the PD Training test.
Together, both findings suggest the higher difficulty of the Exclusion instructions, due to the additional response-reversal requirement.

In addition to these findings, another unexpected effect of PD Instruction was found on the subsequent valence-memory task: 
For Attitude responses, uncertainty was reduced (Fig. \@ref(fig:pd-usmem2) left), and the influence of pre-experimental valence was increased for nonpaired valent stimuli (see Appendix).
In sum, several measures (proportion of Memory responses, RT and accuracy of Attitude responses, valence-memory responses) differed between Inclusion and Exclusion instructions, pointing to a violation of the invariance assumption.

## Summary and Discussion

Experiment 1 largely confirmed expectations:
First, the modified procedure was successfully validated as a measure of subjective memory and attitudes:
Subjective *remember* states were more likely for paired than non-paired CSs; and PD Attitude responses reflected pre-learning evaluative ratings.

Second, mnemonic introspection in the PD procedure was far from perfect:
We found substantial proportions of introspective false alarms (i.e., non-paired stimuli classified as "remembered" or "remembered" paired stimuli with inaccurate Memory responses) as well as introspective misses (i.e., paired stimuli classified as "not remembered" for which veridical memory was nevertheless found).

Third, results suggests that cognitive processes differed between Inclusion and Exclusion:
Under Exclusion instructions, there were more Memory responses; and Attitude responses were more difficult (Attitude responses took longer; and they were less accurate for a subgroup of participants).
In addition, performing the Exclusion task appeared to have induced carry-over effects to subsequent tasks: 
In the Exclusion group, the proportion of "dont know" valence-memory responses was much lower, in particular for Attitude responses, 
and the valence-memory responses more strongly reflected pre-rated evaluations in the Exclusion group for those cases.
This suggests that the Exclusion task may have induced a strategy of using pre-existing evaluations to respond to memory questions about US valence.

Experiment 1 also had several limitations.
First, pairing memory and (pre-existing) attitude were confounded: 
Only neutrally pre-rated CSs were paired with (positive or negative) USs; valent CSs were never paired.
Second, US identity memory was at chance; as briefly discussed above, this was likely due to the relatively large number of unrepeated pairings. 
^[Each specific CS-US pairing was presented only once, while each stimulus (CS or US) was also paired with other stimuli (i.e., each CS was paired with 8 different USs; each US was paired, on average, with 2 different CSs).]
Third, the modified PD procedure is limited in that it obtains incomplete information about subjective memory states: 
It assesses objective memory only in the "remember" state; and attitudes only in the "don't remember" state.
Finally, although preregistered, we tested several assumptions of the original procedure (some in multiple tests), which may have inflated the probability to erroneously conclude that at least one assumption is violated.
